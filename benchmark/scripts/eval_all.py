"""
æ‰¹é‡è¯„ä¼°æ‰€æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
- è‡ªåŠ¨ä»é…ç½®è¯»å–æ¨¡å‹è·¯å¾„
- ç”Ÿæˆè¯¦ç»†å¯¹æ¯”æŠ¥å‘Š
"""
import yaml
from pathlib import Path
from ultralytics import YOLO
import pandas as pd
import time
import torch
import json

def load_faster_rcnn_results():
    """åŠ è½½ Faster R-CNN è¯„ä¼°ç»“æœ (å¦‚æœå­˜åœ¨)"""
    results_path = Path('/home/aiuser/work/ultralytics/runs/detect/benchmark/faster_rcnn/eval_results.json')
    if results_path.exists():
        with open(results_path, 'r') as f:
            data = json.load(f)
            return {
                'Model': 'Faster R-CNN',
                'mAP@0.5': f"{data['mAP50']:.4f}",
                'mAP@0.5:0.95': f"{data['mAP50-95']:.4f}",
                'Precision': f"{data.get('precision', 0):.4f}",
                'Recall': "N/A",  # Detectron2 ä¸ç›´æ¥æä¾›
                'Speed(ms)': "N/A",  # éœ€è¦å•ç‹¬æµ‹è¯•
                'FPS': "N/A",
                'Params(M)': f"{data['params']:.2f}",
                'GFLOPs': "N/A"
            }
    return None

def load_config(config_path='../configs/benchmark_config.yaml'):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = Path(__file__).parent / config_path
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def find_best_weights(model_name, model_config, common_config):
    """æŸ¥æ‰¾æ¨¡å‹çš„æœ€ä½³æƒé‡æ–‡ä»¶"""
    # 1. æ£€æŸ¥æ˜¯å¦æœ‰existing_results
    if 'existing_results' in model_config:
        weight_path = Path(model_config['existing_results']) / 'weights' / 'best.pt'
        if weight_path.exists():
            return weight_path

    # 2. åœ¨benchmarkç›®å½•ä¸‹æŸ¥æ‰¾
    project_dir = Path(common_config['project'])
    exp_name = model_config.get('name', model_name)
    weight_path = project_dir / exp_name / 'weights' / 'best.pt'
    if weight_path.exists():
        return weight_path

    return None

def evaluate_model(model_name, weight_path, dataset_config):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"è¯„ä¼°æ¨¡å‹: {model_name}")
    print(f"æƒé‡æ–‡ä»¶: {weight_path}")
    print(f"{'='*60}")

    if not weight_path or not weight_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶")
        return None

    # åŠ è½½æ¨¡å‹
    model = YOLO(str(weight_path))

    # éªŒè¯é›†è¯„ä¼°
    print("ğŸ“Š å¼€å§‹éªŒè¯é›†è¯„ä¼°...")
    metrics = model.val(
        data=dataset_config['path'],
        split='val',
        imgsz=640,
        batch=16,
        verbose=False
    )

    # æ¨ç†é€Ÿåº¦æµ‹è¯•
    print("âš¡ æµ‹è¯•æ¨ç†é€Ÿåº¦...")
    # ä½¿ç”¨éªŒè¯é›†å›¾ç‰‡æµ‹è¯•
    val_dir = Path(dataset_config['path']).parent / 'valid' / 'images'
    if not val_dir.exists():
        # å°è¯•å¦ä¸€ä¸ªå¸¸è§è·¯å¾„
        val_dir = Path(dataset_config['path']).parent / 'val' / 'images'

    if val_dir.exists():
        # é¢„çƒ­
        _ = model.predict(
            source=str(val_dir),
            imgsz=640,
            save=False,
            verbose=False,
            stream=True,
            max_det=300
        )

        # æ­£å¼æµ‹é€Ÿ
        results_list = list(model.predict(
            source=str(val_dir),
            imgsz=640,
            save=False,
            verbose=False,
            stream=True,
            max_det=300
        ))

        # è®¡ç®—å¹³å‡é€Ÿåº¦
        total_time = sum(r.speed['inference'] for r in results_list)
        avg_speed = total_time / len(results_list) if results_list else 0
    else:
        avg_speed = metrics.speed['inference']

    # è®¡ç®—FPS
    fps = 1000.0 / avg_speed if avg_speed > 0 else 0

    # æ¨¡å‹å‚æ•°ç»Ÿè®¡
    params = sum(p.numel() for p in model.model.parameters()) / 1e6

    # è®¡ç®—GFLOPs
    try:
        # YOLOæ¨¡å‹é€šå¸¸æœ‰flopså±æ€§
        gflops = model.model.flops / 1e9 if hasattr(model.model, 'flops') else 0
        if gflops == 0:
            # å¤‡ç”¨æ–¹æ³•ï¼šæ‰‹åŠ¨è®¡ç®—
            from ultralytics.utils.ops import Profile
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥æ›´ç²¾ç¡®
            gflops = 0  # æš‚æ—¶è®¾ä¸º0
    except:
        gflops = 0

    # æ”¶é›†ç»“æœ
    results = {
        'Model': model_name,
        'mAP@0.5': f"{metrics.box.map50:.4f}",
        'mAP@0.5:0.95': f"{metrics.box.map:.4f}",
        'Precision': f"{metrics.box.mp:.4f}",
        'Recall': f"{metrics.box.mr:.4f}",
        'Speed(ms)': f"{avg_speed:.2f}",
        'FPS': f"{fps:.1f}",
        'Params(M)': f"{params:.2f}",
        'GFLOPs': f"{gflops:.2f}" if gflops > 0 else "N/A"
    }

    print(f"\nâœ… {model_name} è¯„ä¼°å®Œæˆ")
    print(f"   mAP@0.5: {results['mAP@0.5']}")
    print(f"   FPS: {results['FPS']}")

    return results

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*60)
    print("ğŸ¯ YOLO Benchmark è¯„ä¼°ç³»ç»Ÿ")
    print("="*60)

    config = load_config()

    all_results = []

    # è¯„ä¼°æ‰€æœ‰ YOLO/RT-DETR æ¨¡å‹
    for model_name, model_config in config['models'].items():
        # è·³è¿‡ Faster R-CNN (å•ç‹¬å¤„ç†)
        if 'framework' in model_config and model_config['framework'] == 'detectron2':
            continue

        weight_path = find_best_weights(
            model_name=model_name,
            model_config=model_config,
            common_config=config['train']
        )

        result = evaluate_model(
            model_name=model_name,
            weight_path=weight_path,
            dataset_config=config['dataset']
        )

        if result:
            all_results.append(result)

    # å°è¯•åŠ è½½ Faster R-CNN ç»“æœ
    faster_rcnn_result = load_faster_rcnn_results()
    if faster_rcnn_result:
        print("\nâœ… æ‰¾åˆ° Faster R-CNN è¯„ä¼°ç»“æœ")
        all_results.append(faster_rcnn_result)
    else:
        print("\nâš ï¸  æœªæ‰¾åˆ° Faster R-CNN ç»“æœ (è¿è¡Œ train_faster_rcnn.py å’Œ eval_faster_rcnn.py æ¥è®­ç»ƒå’Œè¯„ä¼°)")

    # ä¿å­˜å’Œæ˜¾ç¤ºç»“æœ
    if all_results:
        df = pd.DataFrame(all_results)

        # ä¿å­˜CSV
        output_dir = Path(__file__).parent.parent / 'results'
        output_dir.mkdir(parents=True, exist_ok=True)
        csv_path = output_dir / 'benchmark_comparison.csv'
        df.to_csv(csv_path, index=False)

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*80)
        print("ğŸ“Š Benchmarkç»“æœå¯¹æ¯”")
        print("="*80)
        print(df.to_string(index=False))
        print("\n" + "="*80)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")
        print("="*80)

        # æ‰“å°æ€§èƒ½æ’å
        print("\nğŸ† æ€§èƒ½æ’å:")
        df_sorted = df.copy()
        df_sorted['mAP@0.5'] = df_sorted['mAP@0.5'].astype(float)
        df_sorted['FPS'] = df_sorted['FPS'].astype(float)

        print("\n   æŒ‰mAP@0.5æ’åº:")
        for idx, row in df_sorted.sort_values('mAP@0.5', ascending=False).iterrows():
            print(f"   {row['Model']:15s} - {row['mAP@0.5']}")

        print("\n   æŒ‰FPSæ’åº:")
        for idx, row in df_sorted.sort_values('FPS', ascending=False).iterrows():
            print(f"   {row['Model']:15s} - {row['FPS']} FPS")
    else:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°å¯è¯„ä¼°çš„æ¨¡å‹")

if __name__ == '__main__':
    main()
